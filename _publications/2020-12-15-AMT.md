---
title: "Amazon SageMaker Automatic Model Tuning: Scalable Black-box Optimization"
collection: publications
permalink: /publication/2020-12-15-AMT
date: 2020-12-15
venue: 'ArXiv'
paperurl: 'https://arxiv.org/pdf/2012.08489'
citation: 'Valerio Perrone, <b>Huibin Shen</b>, Aida Zolic, Iaroslav Shcherbatyi, Amr Ahmed, Tanya Bansal, Michele Donini, Fela Winkelmolen, Rodolphe Jenatton, Jean Baptiste Faddoul, Barbara Pogorzelska, Miroslav Miladinovic, Krishnaram Kenthapadi, Matthias Seeger, CÃ©dric Archambeau. (2020). &quot;Amazon SageMaker Automatic Model Tuning: Scalable Black-box Optimization&quot; <i>arXiv preprint arXiv:2012.08489</i>'
---

[[pdf]](https://arxiv.org/pdf/2012.08489.pdf) [[arXiv]](https://arxiv.org/pdf/2012.08489) [[bib]](http://huibinshen.github.io/files/2015-12-15-AMT.bib)

## Abstract
Autoregressive models use the chain rule to define a joint probability distribution as a product of conditionals. These conditionals need to be normalized, imposing constraints on the functional families that can be used. To increase flexibility, we propose autoregressive conditional score models (AR-CSM) and parameterize the joint distribution in terms of the derivatives of univariate log-conditionals (scores), which need not be normalized. To train AR-CSM, we introduce a new divergence between distributions named Composite Score Matching (CSM). For AR-CSM models, this divergence between data and model distributions can be computed and optimized efficiently, requiring no expensive sampling or adversarial training. Compared to previous score matching algorithms, our method is more scalable to high dimensional data and more stable to optimize. We show with extensive experimental results that it can be applied to density estimation on synthetic data, image generation, image denoising, and training latent variable models with implicit encoders.